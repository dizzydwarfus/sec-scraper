{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    sys.path.remove('/Users/dizzydwarfus/Dev/sec_scraper')\n",
    "    sys.path.remove('C:\\\\Users\\\\lianz\\\\Python\\\\sec_scraper')\n",
    "except ValueError as e:\n",
    "    print(f'{e}')\n",
    "finally:\n",
    "    sys.path.append('/Users/dizzydwarfus/Dev/sec_scraper')\n",
    "    sys.path.append('C:\\\\Users\\\\lianz\\\\Python\\\\sec_scraper')\n",
    "\n",
    "# Built-in libraries\n",
    "import os\n",
    "import datetime as dt\n",
    "import re\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Third-party libraries\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from bs4.element import Tag\n",
    "\n",
    "# Internal imports\n",
    "from sec_class import SECData, TickerData\n",
    "from utils._dataclasses import Facts, Context, LinkLabels\n",
    "from utils._mapping import STANDARD_NAME_MAPPING\n",
    "from utils._utils import reverse_standard_mapping, get_filing_facts, translate_labels_to_standard_names, clean_values_in_facts, clean_values_in_segment, segment_breakdown_levels, split_facts_into_start_instant, get_monthly_period\n",
    "from utils.database._connector import SECDatabase\n",
    "from utils._logger import MyLogger\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize variables for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()\n",
    "mongo = SECDatabase(os.getenv('mongodb_sec'))\n",
    "ticker = TickerData(ticker='BLK')\n",
    "\n",
    "file = ticker.filings.loc[ticker.filings['form'] == '10-K'].iloc[0]\n",
    "accessionNumber = file.get('accessionNumber')\n",
    "folder_url = file.get('folder_url')\n",
    "file_url = file.get('file_url')\n",
    "soup = ticker.get_file_data(file_url=file_url)\n",
    "index_df = ticker.get_filing_folder_index(folder_url=folder_url)\n",
    "\n",
    "# ticker.scrape_logger.info(\n",
    "#     file.get('filingDate').strftime('%Y-%m-%d') + ': ' + folder_url)\n",
    "\n",
    "# start_date = dt.datetime(2022, 1, 1) # after XBRL implementation\n",
    "\n",
    "# query = {\n",
    "#     'cik': ticker.cik,\n",
    "#     'form': {'$in': ['10-K']},\n",
    "#     'filingDate': {'$gte': start_date},\n",
    "# }\n",
    "\n",
    "# filings_to_scrape = [i for i in mongo.tickerfilings.find(query).sort('filingDate', 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to insert submission, filings, and facts for each filing into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()\n",
    "sic_dict = sec.get_sic_list()\n",
    "mongo = SECDatabase(connection_string=os.getenv('mongodb_sec'))\n",
    "\n",
    "failed_submissions = []\n",
    "failed_filings = []\n",
    "failed_facts = []\n",
    "\n",
    "with trange(len(sec.cik_list['ticker'][:50]), desc='Instantiating ticker...',) as t:\n",
    "    for item in t:\n",
    "        ticker = sec.cik_list['ticker'].iloc[item] # Get ticker from cik_list\n",
    "        t.set_postfix(ticker=ticker, cik=sec.cik_list['cik_str'].iloc[item])\n",
    "\n",
    "        # Initialize and instantiate TickerData object\n",
    "        try:\n",
    "            symbol = TickerData(ticker=ticker)\n",
    "            cik = symbol.cik # get cik of ticker\n",
    "            symbol.submissions['lastUpdated'] = dt.datetime.now()\n",
    "            symbol.submissions['office'] = mongo.sicdb.find_one({'_id': symbol.submissions['sic']})['Office']\n",
    "            sec.scrape_logger.info(f'{t}')\n",
    "            sec.scrape_logger.info(f'\\nInstantiated {symbol}...')\n",
    "        except Exception as e:\n",
    "            sec.scrape_logger.info(f'{t}')\n",
    "            sec.scrape_logger.error(f'Failed to instantiate {ticker} with cik {cik}...{e}')\n",
    "            continue\n",
    "\n",
    "        filings = symbol.submissions.pop('filings')\n",
    "        # print(filings)\n",
    "        # Insert submissions to TickerData collection\n",
    "        inserted_submission = mongo.insert_submission(submission=symbol._submissions)\n",
    "        if inserted_submission is not None:\n",
    "            failed_submissions.append(inserted_submission)\n",
    "\n",
    "        # Insert filings to TickerFilings collection\n",
    "        inserted_filing = mongo.insert_filings(cik=cik, filings=filings)\n",
    "        if inserted_filing is not None:\n",
    "            failed_filings.append(inserted_filing)\n",
    "\n",
    "        # # Insert facts to Facts collection\n",
    "        # for doc in filings:\n",
    "        #     doc['lastUpdated'] = dt.datetime.now()\n",
    "\n",
    "        #     if doc['form'] == '10-Q' or doc['form'] == '10-K':\n",
    "        #         try:\n",
    "        #             facts = symbol.get_facts_for_each_filing(doc)\n",
    "        #             inserted_facts = mongo.insert_facts(accession=doc['accessionNumber'], facts=facts)\n",
    "        #             if inserted_facts is not None:\n",
    "        #                 failed_facts.append(inserted_facts)\n",
    "        #         except Exception as e:\n",
    "        #             sec.scrape_logger.error(f'TickerData().get_facts_for_each_filing() function failed for {doc[\"accessionNumber\"]}...{e}')\n",
    "        #             failed_facts.append(doc['accessionNumber'])\n",
    "            \n",
    "        sec.scrape_logger.info(f'Successfully updated {ticker}({cik})...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test getting labels from _lab.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ticker.filings.loc[ticker.filings['form'] == '10-K'].iloc[13]\n",
    "accessionNumber = file.get('accessionNumber')\n",
    "folder_url = file.get('folder_url')\n",
    "file_url = file.get('file_url')\n",
    "soup = ticker.get_file_data(file_url=file_url)\n",
    "index_df = ticker.get_filing_folder_index(folder_url=folder_url)\n",
    "\n",
    "labels = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                scrape_file_extension='_lab').query(\"`xlink:type` == 'resource'\")\n",
    "labels['xlink:role'] = labels['xlink:role'].str.split(\n",
    "    '/').apply(lambda x: x[-1])\n",
    "labels['xlink:labelOriginal'] = labels['xlink:label']\n",
    "labels['xlink:label'] = labels['xlink:label']\\\n",
    "    .str.replace('(lab_)|(_en-US)', '', regex=True)\\\n",
    "        .str.split('_')\\\n",
    "            .apply(lambda x: ':'.join(x[:2]))\\\n",
    "    .str.lower()\n",
    "labels['accessionNumber'] = accessionNumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test get_filing_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()\n",
    "mongo = SECDatabase(os.getenv('mongodb_sec'))\n",
    "ticker = TickerData(ticker='MSFT')\n",
    "forms = ['10-K']\n",
    "start_year = 2023\n",
    "end_year = 2023\n",
    "\n",
    "filing_available = ticker.filings[(ticker.filings['form'].isin(forms)) & (\n",
    "    ticker.filings['filingDate'].dt.year >= start_year) & (ticker.filings['filingDate'].dt.year <= end_year)]\n",
    "filing_available = filing_available.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels, all_calc, all_defn, all_context, all_facts, all_metalinks, all_merged_facts, failed_folders = get_filing_facts(ticker=ticker, filings_to_scrape=filing_available, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all_labels, all_calc, all_defn to xlsx on different sheets\n",
    "# with pd.ExcelWriter(f'././data/{ticker.ticker}_all_data.xlsx') as writer:\n",
    "#     all_facts.to_excel(writer, sheet_name='facts', index=False)\n",
    "#     all_context.to_excel(writer, sheet_name='context', index=False)\n",
    "#     all_labels.to_excel(writer, sheet_name='labels', index=False)\n",
    "#     all_merged_facts.to_excel(writer, sheet_name='merged_facts', index=False)\n",
    "#     all_calc.to_excel(writer, sheet_name='calc', index=False)\n",
    "#     all_defn.to_excel(writer, sheet_name='defn', index=False)\n",
    "#     all_metalinks.to_excel(writer, sheet_name='metalinks', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = clean_values_in_facts(all_merged_facts)\n",
    "final_df = clean_values_in_segment(final_df, labels_df=all_labels)\n",
    "final_df = get_monthly_period(final_df)\n",
    "final_df = translate_labels_to_standard_names(final_df, standard_name_mapping=STANDARD_NAME_MAPPING)\n",
    "# final_df, start_end, instant = split_facts_into_start_instant(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Facts Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Facts:\n",
    "    fact_tag: Tag\n",
    "\n",
    "    @property\n",
    "    def factName(self) -> Union[str, None]:\n",
    "        \"\"\"Get factName\n",
    "\n",
    "        Returns:\n",
    "            str: factName\n",
    "        \"\"\"\n",
    "        return self.fact_tag.name\n",
    "\n",
    "    @property\n",
    "    def factId(self) -> Union[str, None]:\n",
    "        \"\"\"Get factId\n",
    "\n",
    "        Returns:\n",
    "            str: factId\n",
    "        \"\"\"\n",
    "        return self.fact_tag.attrs.get('id')\n",
    "\n",
    "    @property\n",
    "    def contextRef(self) -> Union[str, None]:\n",
    "        \"\"\"Get contextRef\n",
    "\n",
    "        Returns:\n",
    "            str: contextRef\n",
    "        \"\"\"\n",
    "        return self.fact_tag.attrs.get('contextref')\n",
    "\n",
    "    @property\n",
    "    def unitRef(self) -> Union[str, None]:\n",
    "        \"\"\"Get unitRef\n",
    "\n",
    "        Returns:\n",
    "            str: unitRef\n",
    "        \"\"\"\n",
    "        return self.fact_tag.attrs.get('unitref')\n",
    "\n",
    "    @property\n",
    "    def decimals(self):\n",
    "        \"\"\"Get decimals\n",
    "\n",
    "        Returns:\n",
    "            str: decimals\n",
    "        \"\"\"\n",
    "        return self.fact_tag.attrs.get('decimals')\n",
    "\n",
    "    @property\n",
    "    def factValue(self) -> Union[str, int, None]:\n",
    "        \"\"\"Get factValue\n",
    "\n",
    "        Returns:\n",
    "            str: factValue\n",
    "        \"\"\"\n",
    "        return self.fact_tag.text\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert facts to dict\n",
    "\n",
    "        Returns:\n",
    "            dict: dict containing facts information\n",
    "        \"\"\"\n",
    "        return dict(factName=self.factName, factId=self.factId, contextRef=self.contextRef, unitRef=self.unitRef, decimals=self.decimals, factValue=self.factValue)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Facts(factName={self.factName}, factId={self.factId}, contextRef={self.contextRef}, unitRef={self.unitRef}, decimals={self.decimals}, factValue={self.factValue})'\n",
    "\n",
    "    def __repr_html__(self):\n",
    "        return f\"\"\"\n",
    "        <div style=\"border: 1px solid #ccc; padding: 10px; margin: 10px;\">\n",
    "            <h3>Facts</h3>\n",
    "            <p><strong>factName:</strong> {self.factName}</p>\n",
    "            <p><strong>factId:</strong> {self.factId}</p>\n",
    "            <p><strong>contextRef:</strong> {self.contextRef}</p>\n",
    "            <p><strong>unitRef:</strong> {self.unitRef}</p>\n",
    "            <p><strong>decimals:</strong> {self.decimals}</p>\n",
    "            <p><strong>factValue:</strong> {self.factValue}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'''factName={self.factName}\n",
    "factId={self.factId}\n",
    "contextRef={self.contextRef}\n",
    "unitRef={self.unitRef}\n",
    "decimals={self.decimals}\n",
    "factValue={self.factValue}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()\n",
    "mongo = SECDatabase(os.getenv('mongodb_sec'))\n",
    "ticker = TickerData(ticker='RTX')\n",
    "forms = ['10-K']\n",
    "\n",
    "file = ticker.filings.loc[ticker.filings['form'].isin(forms)].iloc[0]\n",
    "accessionNumber = file.get('accessionNumber')\n",
    "folder_url = file.get('folder_url')\n",
    "file_url = file.get('file_url')\n",
    "soup = ticker.get_file_data(file_url=file_url)\n",
    "index_df = ticker.get_filing_folder_index(folder_url=folder_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_list = []\n",
    "facts = ticker.search_facts(soup=soup)\n",
    "for fact_tag in facts:\n",
    "    print(fact_tag.prettify())\n",
    "    print(fact_tag.text)\n",
    "    # fact_tag = Facts(fact_tag=fact_tag)\n",
    "    break\n",
    "    # facts_list.append(Facts(fact_tag=fact_tag).to_dict())\n",
    "\n",
    "# pd.DataFrame(facts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Context Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Context:\n",
    "    context_tag: Tag\n",
    "    entity_pattern: str = \".*identifier.*\"\n",
    "    startDate_pattern: str = \".*startdate.*\"\n",
    "    endDate_pattern: str = \".*enddate.*\"\n",
    "    instant_pattern: str = \".*instant.*\"\n",
    "    segment_pattern: str = \".*segment.*\"\n",
    "    segment_breakdown_pattern: str = \".*xbrldi:.*\"\n",
    "\n",
    "    @property\n",
    "    def contextId(self) -> str:\n",
    "        \"\"\"Get contextId\n",
    "\n",
    "        Returns:\n",
    "            str: contextId\n",
    "        \"\"\"\n",
    "        return self.context_tag.attrs.get('id')\n",
    "    \n",
    "    @property\n",
    "    def entity(self) -> Union[str, None]:\n",
    "        pattern = re.compile(self.entity_pattern)\n",
    "        result = self.context_tag.find(pattern)\n",
    "        return result.text if result is not None else None\n",
    "    \n",
    "    @property\n",
    "    def startDate(self) -> str:\n",
    "        return self.search_dates(self.startDate_pattern)\n",
    "    \n",
    "    @property\n",
    "    def endDate(self) -> str:\n",
    "        return self.search_dates(self.endDate_pattern)\n",
    "    \n",
    "    @property\n",
    "    def instant(self) -> str:\n",
    "        return self.search_dates(self.instant_pattern)\n",
    "    \n",
    "    @property\n",
    "    def segment(self) -> Union[dict, None]:\n",
    "        \"\"\"Get segments and tags classifying the segment and store in dict\n",
    "\n",
    "        Returns:\n",
    "            dict: dict containing segment and tags classifying the segment\n",
    "        \"\"\"\n",
    "        segment_pattern = re.compile(self.segment_pattern)\n",
    "        segment_breakdown_pattern = re.compile(self.segment_breakdown_pattern)\n",
    "\n",
    "        segment = self.context_tag.find(segment_pattern)\n",
    "\n",
    "        if segment is None:\n",
    "            return None\n",
    "\n",
    "        segment_dict = {}\n",
    "\n",
    "        segment_breakdown = segment.find_all(segment_breakdown_pattern)\n",
    "\n",
    "        for i in segment_breakdown:\n",
    "            segment_dict[i.attrs.get('dimension')] = i.text\n",
    "\n",
    "        return segment_dict\n",
    "\n",
    "    def search_dates(self, pattern: str) -> Union[str, None]:\n",
    "        \"\"\"Search for pattern in context tag\n",
    "\n",
    "        Args:\n",
    "            pattern (str): pattern to search for\n",
    "\n",
    "        Returns:\n",
    "            Union[str, None]: result of search\n",
    "        \"\"\"\n",
    "        pattern = re.compile(pattern)\n",
    "        result = self.context_tag.find(pattern)\n",
    "\n",
    "        if result is None:\n",
    "            return None\n",
    "        \n",
    "        result = result.text\n",
    "\n",
    "        if result == '':\n",
    "            return None\n",
    "        \n",
    "        return dt.datetime.strptime(result, '%Y-%m-%d')\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert context to dict\n",
    "\n",
    "        Returns:\n",
    "            dict: dict containing context information\n",
    "        \"\"\"\n",
    "        context_dict = {\n",
    "            'contextId': self.contextId,\n",
    "            'entity': self.entity,\n",
    "            'segment': self.segment,\n",
    "            'startDate': self.startDate,\n",
    "            'endDate': self.endDate,\n",
    "            'instant': self.instant,\n",
    "            'segmentLength': self.get_segment_length()\n",
    "\n",
    "        }\n",
    "        return context_dict\n",
    "\n",
    "    def get_segment_length(self) -> int:\n",
    "        \"\"\"Get length of segment\n",
    "\n",
    "        Returns:\n",
    "            int: length of segment\n",
    "        \"\"\"\n",
    "        segment = self.context_tag.find(re.compile(\".*segment.*\"))\n",
    "\n",
    "        if segment is None:\n",
    "            return 0\n",
    "\n",
    "        return len(segment)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Context(contextId={self.contextId}, entity={self.entity}, segment={self.segment}, startDate={self.startDate}, endDate={self.endDate}, instant={self.instant})'\n",
    "\n",
    "    def __repr_html__(self):\n",
    "        return f\"\"\"\n",
    "        <div style=\"border: 1px solid #ccc; padding: 10px; margin: 10px;\">\n",
    "            <h3>Context</h3>\n",
    "            <p><strong>contextId:</strong> {self.contextId}</p>\n",
    "            <p><strong>entity:</strong> {self.entity}</p>\n",
    "            <p><strong>segment:</strong> {self.segment}</p>\n",
    "            <p><strong>startDate:</strong> {self.startDate}</p>\n",
    "            <p><strong>endDate:</strong> {self.endDate}</p>\n",
    "            <p><strong>instant:</strong> {self.instant}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'''contextId={self.contextId}\n",
    "entity={self.entity}\n",
    "segment={self.segment}\n",
    "startDate={self.startDate}\n",
    "endDate={self.endDate}\n",
    "instant={self.instant}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()\n",
    "mongo = SECDatabase(os.getenv('mongodb_sec'))\n",
    "ticker = TickerData(ticker='RTX')\n",
    "\n",
    "file = ticker.filings.loc[ticker.filings['form'] == '10-K'].iloc[0]\n",
    "accessionNumber = file.get('accessionNumber')\n",
    "folder_url = file.get('folder_url')\n",
    "file_url = file.get('file_url')\n",
    "soup = ticker.get_file_data(file_url=file_url)\n",
    "index_df = ticker.get_filing_folder_index(folder_url=folder_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = []\n",
    "contexts = ticker.search_context(soup=soup)\n",
    "for tag in contexts[:]:\n",
    "    parsed_tag = Context(context_tag=tag)\n",
    "    context_list.append(parsed_tag.to_dict())\n",
    "\n",
    "context_df = pd.DataFrame(context_list).drop_duplicates(subset=['contextId'],keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test processing segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_breakdown_levels(final_df: pd.DataFrame) -> int:\n",
    "    dict_len = 0\n",
    "    for i in final_df['segment']:\n",
    "        if isinstance(i, dict):\n",
    "            curr_len = len(list(i.items()))\n",
    "            if  curr_len > dict_len:\n",
    "                dict_len = curr_len\n",
    "                if curr_len > 1:\n",
    "                    print(list(i.items()))\n",
    "\n",
    "    return dict_len\n",
    "\n",
    "segment_breakdown_levels(all_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_facts = all_merged_facts.copy()\n",
    "labels_df = all_labels.copy()\n",
    "from typing import Literal\n",
    "\n",
    "def join_segments(x: dict, segment_type: Literal['key','value']) -> str:\n",
    "    if x is not None and isinstance(x, dict):\n",
    "        try:\n",
    "            if segment_type == 'key':\n",
    "                result = \", \".join(list(x.keys()))\n",
    "            elif segment_type == 'value':\n",
    "                result = \", \".join(list(x.values()))\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e} on {x}')\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "labels_df = labels_df.query(\"`xlink:role` == 'label'\")[['xlink:label', 'labelText']]\\\n",
    "    .set_index('xlink:label')\\\n",
    "        .to_dict()['labelText']\n",
    "\n",
    "merged_facts['segment_modified'] = merged_facts['segment'].apply(lambda x: {labels_df.get(i.lower()): labels_df.get(j.lower()) for i, j in x.items()} if isinstance(x, dict) else None)\n",
    "\n",
    "merged_facts['segmentAxis'] = merged_facts['segment_modified']\\\n",
    "    .apply(lambda x: join_segments(x, segment_type='key'))\n",
    "\n",
    "merged_facts['segmentValue'] = merged_facts['segment_modified']\\\n",
    "    .apply(lambda x: join_segments(x, segment_type='value'))\n",
    "\n",
    "merged_facts.drop(['segment', 'segment_modified'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test merging labels with labels from xbrl us-gaap xsd document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbrl_us_gaap = 'http://xbrl.fasb.org/us-gaap/2024/elts/us-gaap-2024.xsd'\n",
    "xbrl_srt = 'http://xbrl.fasb.org/srt/2024/elts/srt-std-2024.xsd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sec.us_gaap_tags\n",
    "labels['id'] = labels['id'].str.split('_', n=1).str.join(':').str.lower()\n",
    "merged_fact_with_label = all_facts.merge(labels, how='left', left_on='factName', right_on='id')\n",
    "[i for i in merged_fact_with_label.loc[merged_fact_with_label['id'].isnull(),'factName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_len_list = []\n",
    "for label in labels['id']:\n",
    "    label_len_dict = {}\n",
    "    label_len_dict['label_name'] = label\n",
    "    label_len_dict['label_len'] = len(label)\n",
    "    label_len_list.append(label_len_dict)\n",
    "\n",
    "pd.DataFrame(label_len_list).sort_values(by='label_len', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse using GPT (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = symbol.search_context(soup)[0]\n",
    "data = {\n",
    "    'id': context['id'],\n",
    "    'entity': {\n",
    "        'identifier': {\n",
    "            'scheme': context.find('identifier')['scheme'],\n",
    "            'value': context.find('identifier').text\n",
    "        }\n",
    "    },\n",
    "    'period': {\n",
    "        'startDate': context.find('startdate').text,\n",
    "        'endDate': context.find('enddate').text\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import XMLOutputParser\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessage,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import json\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "parser = XMLOutputParser(tags=['id', 'entity', 'period'])\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that parses XML files for a company's financial statements from the SEC Edgar database.\"\n",
    "                \"The XML content will be provided by the user.\"\n",
    "                \"You will parse the output and return it in the json format.\"\n",
    "                \"{format_instructions}\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{xml}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "context_list = []\n",
    "total_cost = 0\n",
    "total_tokens = 0\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "with trange(len(contexts[:]), desc='Scraping contexts...') as t:\n",
    "    for i in t:\n",
    "        with get_openai_callback() as cb:\n",
    "            t.set_postfix(context=contexts[i].attrs.get('id'))\n",
    "            output = llm(template.format_messages(format_instructions=parser.get_format_instructions(), xml=contexts[i]))\n",
    "            total_cost += cb.total_cost\n",
    "            total_tokens += cb.total_tokens\n",
    "            context_list.append(json.loads(output.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'browser'\n",
    "pio.renderers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = final_df\n",
    "metric_df = metric_df.loc[\\\n",
    "    (metric_df['labelText'].str.contains('Sales Revenue |'))\\\n",
    "    & (metric_df['segmentAxis'] == 'Statement Business Segments Axis')\\\n",
    "].sort_values(by=['labelText', 'segmentAxis', 'segmentValue', 'startDate', 'endDate'])\n",
    "print(metric_df['segmentValue'].unique())\n",
    "\n",
    "metric_df = metric_df.loc[\\\n",
    "    (metric_df['segmentValue'] == 'Europe Member')\\\n",
    "]\n",
    "metric_df = metric_df.drop_duplicates(subset=['labelText', 'segmentAxis', 'segmentValue', 'startDate', 'endDate'], keep='last',)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['labelText'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot\n",
    "fig = px.line(metric_df, x='endDate', y='factValue',\n",
    "                color='labelText', line_group='labelText',\n",
    "                #   hover_data={'change': ':,'},\n",
    "                )\n",
    "# Overlay a scatter plot for the individual points\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=metric_df['endDate'],\n",
    "        y=metric_df['factValue'],\n",
    "        mode='markers',\n",
    "        # marker=dict(\n",
    "        #     color=metric_df['color'].map(\n",
    "        #         {'increase': 'green', 'decrease': 'red', 'neutral': 'grey'}),\n",
    "        #     size=15,\n",
    "        #     symbol=metric_df['color'].map(\n",
    "        #         {'increase': 'triangle-up', 'decrease': 'triangle-down', 'neutral': 'circle'})\n",
    "        # ),\n",
    "        hoverinfo='skip',\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "# for trace in fig.data:\n",
    "#     print(trace)\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title='Metrics over time',\n",
    "    xaxis_title='End Date',\n",
    "    yaxis_title='Value',\n",
    "    legend_title='Segment Axis',\n",
    "    font=dict(\n",
    "        family='Courier New, monospace',\n",
    "        size=18,\n",
    "        color='RebeccaPurple'\n",
    "    ),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(autorange=True)\n",
    "fig.update_yaxes(autorange=True, rangemode=\"tozero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Insert Facts to TickerFilings Collection\n",
    "\n",
    "Insert when AccessionNumber is the same\n",
    "- DECIDE: only insert merged facts\n",
    "- DECIDE: or insert raw facts, context, labels before merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    sys.path.remove('/Users/dizzydwarfus/Dev/sec_scraper')\n",
    "    sys.path.remove('C:\\\\Users\\\\lianz\\\\Python\\\\sec_scraper')\n",
    "except ValueError as e:\n",
    "    print(f'{e}')\n",
    "finally:\n",
    "    sys.path.append('/Users/dizzydwarfus/Dev/sec_scraper')\n",
    "    sys.path.append('C:\\\\Users\\\\lianz\\\\Python\\\\sec_scraper')\n",
    "\n",
    "# Built-in libraries\n",
    "import os\n",
    "import datetime as dt\n",
    "import re\n",
    "from typing import Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Third-party libraries\n",
    "from tqdm import trange\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from bs4.element import Tag\n",
    "\n",
    "# Internal imports\n",
    "from sec_class import SECData, TickerData\n",
    "from utils._dataclasses import Facts, Context, LinkLabels\n",
    "from utils._mapping import STANDARD_NAME_MAPPING\n",
    "from utils._utils import reverse_standard_mapping, get_filing_facts, translate_labels_to_standard_names, clean_values_in_facts, clean_values_in_segment, segment_breakdown_levels, split_facts_into_start_instant, get_monthly_period\n",
    "from utils.database._connector import SECDatabase\n",
    "from utils._logger import MyLogger\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sec = SECData()\n",
    "mongo = SECDatabase(connection_string=os.getenv('mongodb_sec'))\n",
    "# ticker = TickerData(ticker='AAPL')\n",
    "# forms = ['10-K', '10-Q']\n",
    "# start_year = 2009\n",
    "# end_year = 2023\n",
    "# filing_available = ticker.filings[(ticker.filings['form'].isin(forms)) & (\n",
    "#     ticker.filings['filingDate'].dt.year >= start_year) & (ticker.filings['filingDate'].dt.year <= end_year)]\n",
    "# filing_available = filing_available.to_dict('records')\n",
    "\n",
    "accessionNumber = \"0000320193-23-000077\"\n",
    "filing = mongo.tickerfilings.find_one({'accessionNumber': accessionNumber})\n",
    "filing_available = [filing]\n",
    "filing_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = pd.DataFrame()\n",
    "all_calc = pd.DataFrame()\n",
    "all_defn = pd.DataFrame()\n",
    "all_context = pd.DataFrame()\n",
    "all_facts = pd.DataFrame()\n",
    "all_metalinks = pd.DataFrame()\n",
    "all_merged_facts = pd.DataFrame()\n",
    "failed_folders = []\n",
    "fact_update_requests = []\n",
    "\n",
    "for file in filing_available:\n",
    "    if (file.get('form') != '10-Q' or file.get('form') != '10-K') and file.get('filingDate') < dt.datetime(2009, 1, 1):\n",
    "        continue\n",
    "\n",
    "    accessionNumber = file.get('accessionNumber')\n",
    "    folder_url = file.get('folder_url')\n",
    "    file_url = file.get('file_url')\n",
    "    ticker.scrape_logger.info(\n",
    "        file.get('filingDate').strftime('%Y-%m-%d') + ': ' + folder_url)\n",
    "\n",
    "    soup = ticker.get_file_data(file_url=file_url)\n",
    "\n",
    "    try:  # Scrape facts\n",
    "        facts_list = []\n",
    "        facts = ticker.search_facts(soup=soup)\n",
    "        for fact_tag in facts:\n",
    "            facts_list.append(Facts(fact_tag=fact_tag).to_dict())\n",
    "        facts_df = pd.DataFrame(facts_list)\n",
    "        # if facts_list != []:\n",
    "        #     facts_df['accessionNumber'] = accessionNumber\n",
    "        #     fact_update_requests.append(mongo.create_facts_update_request(accessionNumber=accessionNumber, facts=facts_list))\n",
    "        \n",
    "        facts_df['accessionNumber'] = accessionNumber\n",
    "        all_facts = pd.concat([all_facts, facts_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        ticker.scrape_logger.error(\n",
    "            f'Failed to scrape facts for {folder_url}...{e}')\n",
    "        failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                error=f'Failed to scrape facts for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "        pass\n",
    "\n",
    "    if len(facts_list) == 0:\n",
    "        ticker.scrape_logger.info(\n",
    "            f'No facts found for {ticker.ticker}({ticker.cik})-{folder_url}...\\n')\n",
    "        continue\n",
    "\n",
    "    try:  # Scrape context\n",
    "        context_list = []\n",
    "        contexts = ticker.search_context(soup=soup)\n",
    "        for tag in contexts:\n",
    "            context_list.append(Context(context_tag=tag).to_dict())\n",
    "        context_df = pd.DataFrame(context_list)\n",
    "        context_df['accessionNumber'] = accessionNumber\n",
    "        all_context = pd.concat(\n",
    "            [all_context, context_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        ticker.scrape_logger.error(\n",
    "            f'Failed to scrape context for {folder_url}...{e}')\n",
    "        failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                error=f'Failed to scrape context for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "        pass\n",
    "\n",
    "    index_df = ticker.get_filing_folder_index(folder_url=folder_url)\n",
    "\n",
    "    try:  # Scrape metalinks\n",
    "        metalinks = ticker.get_metalinks(\n",
    "            folder_url=folder_url + '/MetaLinks.json')\n",
    "        metalinks['accessionNumber'] = accessionNumber\n",
    "        all_metalinks = pd.concat(\n",
    "            [all_metalinks, metalinks], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        ticker.scrape_logger.error(\n",
    "            f'Failed to scrape metalinks for {folder_url}...{e}')\n",
    "        failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                error=f'Failed to scrape metalinks for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "        pass\n",
    "\n",
    "    try:  # Scrape labels\n",
    "        labels = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                        scrape_file_extension='_lab').query(\"`xlink:type` == 'resource'\")\n",
    "        labels['xlink:role'] = labels['xlink:role'].str.split(\n",
    "            '/').apply(lambda x: x[-1])\n",
    "        labels['xlink:labelOriginal'] = labels['xlink:label']\n",
    "        labels['xlink:label'] = labels['xlink:label']\\\n",
    "            .str.replace('(lab_)|(_en-US)', '', regex=True)\\\n",
    "                .str.split('_')\\\n",
    "                    .apply(lambda x: ':'.join(x[:2]))\\\n",
    "            .str.lower()\n",
    "        labels['accessionNumber'] = accessionNumber\n",
    "        all_labels = pd.concat([all_labels, labels], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        ticker.scrape_logger.error(\n",
    "            f'Failed to scrape labels for {folder_url}...{e}')\n",
    "        failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                error=f'Failed to scrape labels for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "        pass\n",
    "\n",
    "    try:  # Scrape calculations\n",
    "        calc = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                    scrape_file_extension='_cal').query(\"`xlink:type` == 'arc'\")\n",
    "        calc['accessionNumber'] = accessionNumber\n",
    "        all_calc = pd.concat([all_calc, calc], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        ticker.scrape_logger.error(\n",
    "            f'Failed to scrape calc for {folder_url}...{e}')\n",
    "        failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                error=f'Failed to scrape calc for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "        pass\n",
    "\n",
    "    try:  # Scrape definitions\n",
    "        defn = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                    scrape_file_extension='_def').query(\"`xlink:type` == 'arc'\")\n",
    "        defn['accessionNumber'] = accessionNumber\n",
    "        all_defn = pd.concat([all_defn, defn], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        ticker.scrape_logger.error(\n",
    "            f'Failed to scrape defn for {folder_url}...{e}')\n",
    "        failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                error=f'Failed to scrape defn for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "        pass\n",
    "\n",
    "    ticker.scrape_logger.info(\n",
    "        f'Merging facts with context and labels. Current facts length: {len(facts_list)}...')\n",
    "    try:\n",
    "        merged_facts = facts_df.merge(context_df, how='left', left_on='contextRef', right_on='contextId')\\\n",
    "            .merge(labels.query(\"`xlink:role` == 'label'\"), how='left', left_on='factName', right_on='xlink:label')\n",
    "        merged_facts = merged_facts.drop(\n",
    "            ['accessionNumber_x', 'accessionNumber_y'], axis=1)\n",
    "\n",
    "        ticker.scrape_logger.info(\n",
    "            f'Successfully merged facts with context and labels. Merged facts length: {len(merged_facts)}...')\n",
    "    except Exception as e:\n",
    "        ticker.scrape_logger.error(\n",
    "            f'Failed to merge facts with context and labels for {folder_url}...{e}')\n",
    "        failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                error=f'Failed to merge facts with context and labels for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "        pass\n",
    "\n",
    "    all_merged_facts = pd.concat(\n",
    "        [all_merged_facts, merged_facts], ignore_index=True)\n",
    "    \n",
    "    ticker.scrape_logger.info(\n",
    "        f'Successfully scraped {ticker.ticker}({ticker.cik})-{folder_url}...\\n')\n",
    "\n",
    "all_merged_facts = all_merged_facts.loc[~all_merged_facts['labelText'].isnull(), [\n",
    "    'labelText', 'segment', 'startDate', 'endDate', 'instant', 'factValue', 'unitRef']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo.tickerfilings.bulk_write(fact_update_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = dt.datetime.now().replace(minute=48)\n",
    "updated_filings = mongo.tickerfilings.find({'lastUpdated': {\"$gte\": now}, 'form': {'$in': ['10-K']}}, sort=[('filingDate', -1)])\n",
    "for filing in updated_filings:\n",
    "    print(filing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_fact = mongo.tickerfilings.find_one({'accessionNumber': '0001193125-10-238044'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance-dashboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
